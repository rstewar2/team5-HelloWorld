{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "#!/usr/bin/python\n",
    "# Let Hadoop know we want to use Python to execute our file\n",
    "\n",
    "import sys\n",
    "# This module provides access to some variables used or \n",
    "# maintained by the interpreter and to functions that interact strongly with the interpreter. It is always available.\n",
    "# https://docs.python.org/3/library/sys.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mvMapper(object):\n",
    "# Turn our records into key value pairs\n",
    "    def __init__(self, infile=sys.stdin, separator=','):\n",
    "    # Prepare three arguments, naming the object, what is our input file, and what is our seperator\n",
    "        self.infile = infile # Rename infile to know what infile we are referencing\n",
    "        self.separator = separator # Rename seperator to know what objects seperator we are referencing\n",
    "        self.vectorize = dict() # create a dictionary variable that is consistent with our other variable naming\n",
    "        for i in range(1,5): self.vectorize[i] = 1/4  # initialize v to 1/#nodes\n",
    "\n",
    "    def map(self): # Here we will actually output our Key, Value pair for the Reducer to take in\n",
    "        for row in self: # Begin looping through the object line by line\n",
    "            columns = row.split(',', 3) # Seperate our rows into columns\n",
    "            source = int(columns[0]) # Set our Source pages to column 0\n",
    "            dest = int(columns[1]) # Set our Destination pages to column 1\n",
    "            prob = float(columns[2]) # Set our probability to column 2. We will need to find a way to create this in part 2.\n",
    "            self.emit(dest, prob * self.vectorize[source]) # Output our Key, Value Pair, (Destination Page, Weighted Vector = 1/rows*probability path)\n",
    "\n",
    "    def status(self, message): # Basic Troubleshooting we may call later\n",
    "        sys.stderr.write(\"reporter:status:{0}\\n\".format(message))\n",
    "\n",
    "    def emit(self, key, value): # Emmitting Formatting\n",
    "        sys.stdout.write('{0}{1}{2}\\n'.format(key, self.separator, value))\n",
    "\n",
    "    def read(self): # Remoce trailing whitespace function\n",
    "        for line in self.infile:\n",
    "            yield line.rstrip()\n",
    "\n",
    "    def __iter__(self): # Set Object as the iterator going through line by line through self that doesn't minus trailing white space\n",
    "        for line in self.read():\n",
    "            yield line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__': # Makes sure that all code is executed at the bottom and make sure that script being imported to make sure that the code is completely done before handing it over to Hadoop, this allows for better debugging.\n",
    "    mvMapper().map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
